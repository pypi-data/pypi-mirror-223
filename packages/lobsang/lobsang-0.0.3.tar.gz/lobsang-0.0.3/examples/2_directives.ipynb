{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lobsang in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: openai in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (0.27.4)\r\n",
      "Requirement already satisfied: python-dotenv in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (0.21.0)\r\n",
      "Requirement already satisfied: jsonschema in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (4.17.3)\r\n",
      "Requirement already satisfied: requests>=2.20 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from openai) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from openai) (4.65.0)\r\n",
      "Requirement already satisfied: aiohttp in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from openai) (3.8.3)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from jsonschema) (22.1.0)\r\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from jsonschema) (0.18.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.7.22)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (6.0.2)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (1.8.1)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (1.2.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lobsang openai python-dotenv jsonschema"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T18:18:50.998569292Z",
     "start_time": "2023-08-06T18:18:50.081610998Z"
    }
   },
   "id": "fe37ead9f09988e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Directives\n",
    "\n",
    "Directives in lobsang can be used to guide a LLM to produce specific output formats. For example, you can use a directive to ask the LLM to produce a JSON object. To do so, a directive generally does two things:\n",
    "\n",
    "1. It embeds / wraps a user message with instructs the LLM. For example, the `JSONDirective` adds a provided JSON schema together with general instructions to the user message.\n",
    "   ```python\n",
    "   original = \"Create a marvel hero\"\n",
    "   #  ‚òùÔ∏è becomes üëá \n",
    "   embedded = \"\"\"Create a marvel hero\n",
    "   \n",
    "    Create a JSON object with the following schema:\n",
    "    ```json\n",
    "    {\n",
    "        <SCHEMA HERE>      \n",
    "    }\n",
    "   \"\"\"\n",
    "    ```\n",
    "2. It tries to parse the LLM output to the desired format. For example, the `JSONDirective` tries to parse the LLM output to a JSON object using the provided schema. It will also validate the output against the schema.\n",
    "   **Note:** If the LLM output is not valid JSON, the directive will not raise an error but add it to the returned `info` object. This is so processing multiple message won't stop if one message fails. (‚ö†Ô∏è May change in the future!)\n",
    "\n",
    "\n",
    "Now let's give it a try! üöÄ But first we have to import some stuff and set up our environment (make sure to update the `.env` file with your own OpenAI API key, see [1_basics](1_basics.ipynb) for more details)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4719339b59076015"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "\"All set! üéâ Let's get started! üöÄ OPENAI_API_KEY=sk-PbRY4M6AH6Xh...\""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from lobsang import Chat, OpenAI\n",
    "from lobsang.directives import JSONDirective\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load OpenAI API key from .env file (please update .env file with your own API key)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert OPENAI_API_KEY, \"Please set OPENAI_API_KEY in .env file\"\n",
    "\n",
    "f\"All set! üéâ Let's get started! üöÄ OPENAI_API_KEY={OPENAI_API_KEY[:15]}...\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T18:18:51.003115787Z",
     "start_time": "2023-08-06T18:18:50.998738321Z"
    }
   },
   "id": "b50acd4f30bfb48d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our example, we want to create a marvel hero and output it as a JSON object. To do so, we have to create a JSON schema for our hero. We need to use the [JSON Schema](https://json-schema.org/) standard for that. The schema below describes a hero with a name, age and powers ü¶∏"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "917c15bad92b71d4"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "hero_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\", \"description\": \"The name of the hero\"},\n",
    "        \"age\": {\"type\": \"integer\", \"minimum\": 0, \"description\": \"The age of the hero\"},\n",
    "        \"powers\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"The powers of the hero\"\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T18:18:51.010135842Z",
     "start_time": "2023-08-06T18:18:51.005033677Z"
    }
   },
   "id": "b2947f522b52f334"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll take a look at embedding a user message with the `JSONDirective`. The `JSONDirective` takes the schema as an argument and embeds it into the user message:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "565d4bfab016a319"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a marvel hero\n",
      "\n",
      "Create a JSON object with the following schema:\n",
      "```json\n",
      "{'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name of the hero'}, 'age': {'type': 'integer', 'minimum': 0, 'description': 'The age of the hero'}, 'powers': {'type': 'array', 'items': {'type': 'string'}, 'description': 'The powers of the hero'}}}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "hero_message = \"Create a marvel hero\"\n",
    "hero_directive = JSONDirective(schema=hero_schema)\n",
    "\n",
    "embedded_message, _ = hero_directive.embed(hero_message)\n",
    "\n",
    "print(embedded_message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T18:18:51.018459729Z",
     "start_time": "2023-08-06T18:18:51.007438018Z"
    }
   },
   "id": "f84eb7c2efb1e27f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ô∏èÔ∏èÔ∏è‚òùÔ∏è As you can see the instructions are added to the user message. When using a json directive in chat, note that the user message is updated with the instructions. This is important to know when using the chat history for further processing (we'll see this in a bit).\n",
    "\n",
    "üëá For now, let's also take a look at how the `JSONDirective` parses the LLM output."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13ae63ed2e0c641f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"name\": \"Spiderman\",\n",
      "    \"age\": 18,\n",
      "    \"powers\": [\n",
      "        \"spider sense\",\n",
      "        \"web slinging\",\n",
      "        \"superhuman strength\"\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Hypothetical LLM output\n",
    "response = \"\"\"\n",
    "Sure, i can create a marvel hero for you.\n",
    "\n",
    "Here you go:\n",
    "```json\n",
    "{\n",
    "    \"name\": \"Spiderman\",\n",
    "    \"age\": 18,\n",
    "    \"powers\": [\"spider sense\", \"web slinging\", \"superhuman strength\"]\n",
    "}\n",
    "```\n",
    "\n",
    "I hope you like it!\n",
    "\"\"\"\n",
    "\n",
    "parsed, info = hero_directive.parse(response)\n",
    "print(parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T18:18:51.057469041Z",
     "start_time": "2023-08-06T18:18:51.016072672Z"
    }
   },
   "id": "fee7a4fcca9cbd6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ô∏èÔ∏èÔ∏è‚òùÔ∏è As you can see, the `JSONDirective` parsed the LLM output to a JSON object and returned a string with a json block, removing the surrounding text. By default, the pruned response is returned as a string (see above), but you can change this behavior by setting `prune=False`. This can be helpful if you want to keep the surrounding text of the LLM output.\n",
    "\n",
    "üëá However, this string representation is not very useful for further processing. Therefore, the `JSONDirective` also returns the parsed object as a python object in the `info` dictionary. Let's take a look at it:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad4d5ef0527bf49c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'original': '\\nSure, i can create a marvel hero for you.\\n\\nHere you go:\\n```json\\n{\\n    \"name\": \"Spiderman\",\\n    \"age\": 18,\\n    \"powers\": [\"spider sense\", \"web slinging\", \"superhuman strength\"]\\n}\\n```\\n\\nI hope you like it!\\n',\n 'directive': JSONDirective,\n 'json': {'name': 'Spiderman',\n  'age': 18,\n  'powers': ['spider sense', 'web slinging', 'superhuman strength']},\n 'error': None,\n 'schema': {'type': 'object',\n  'properties': {'name': {'type': 'string',\n    'description': 'The name of the hero'},\n   'age': {'type': 'integer',\n    'minimum': 0,\n    'description': 'The age of the hero'},\n   'powers': {'type': 'array',\n    'items': {'type': 'string'},\n    'description': 'The powers of the hero'}}}}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T18:18:51.057869046Z",
     "start_time": "2023-08-06T18:18:51.057266969Z"
    }
   },
   "id": "7d170d4caf50ddf8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ô∏èÔ∏èÔ∏è‚òùÔ∏è In the info dictionary, you can find the parsed json object under `json`. You can also find the original message under `original`. In our case, parsing was successful, so `error` is `None`. If parsing fails, the `error` key will contain the error message.\n",
    "\n",
    "**Note:** If an error occurs, the json directive will always return the original message. No matter whether `prune` is set to `True` or `False`. This is because since parsing failed, there is no point in pruning the message and replacing it with `None` or an error message would break the chat history.\n",
    "\n",
    "üëá Ok, so now we know how to embed a user message and parse the LLM output. It's time to put it all together and create a chat with a `JSONDirective`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "943096b22fe1f5"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Create a marvel hero\n",
      "\n",
      "Create a JSON object with the following schema:\n",
      "```json\n",
      "{'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name of the hero'}, 'age': {'type': 'integer', 'minimum': 0, 'description': 'The age of the hero'}, 'powers': {'type': 'array', 'items': {'type': 'string'}, 'description': 'The powers of the hero'}}}\n",
      "```\n",
      "\n",
      "ASSISTANT: ```json\n",
      "{\n",
      "    \"name\": \"Iron Man\",\n",
      "    \"age\": 40,\n",
      "    \"powers\": [\n",
      "        \"Superhuman strength\",\n",
      "        \"Flight\",\n",
      "        \"Energy blasts\"\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Create a chat instance with OpenAI LLM\n",
    "chat = Chat(llm=OpenAI(api_key=OPENAI_API_KEY))\n",
    "\n",
    "# Initialize the JSONDirective again (we could also reuse the one from above, we will use the same schema though)\n",
    "hero_directive = JSONDirective(schema=hero_schema)\n",
    "\n",
    "messages = [\n",
    "    \"Create a marvel hero\",\n",
    "    hero_directive,  # üëà We use the JSONDirective from above (the chat instance will use the directive for the corresponding user message, i.e. the message one index before)\n",
    "]\n",
    "\n",
    "# Let's chat!\n",
    "chat.run(messages)\n",
    "\n",
    "print(chat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T18:18:53.633219924Z",
     "start_time": "2023-08-06T18:18:51.057432712Z"
    }
   },
   "id": "f00b8639d594958d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ô∏èÔ∏èÔ∏èÔ∏è‚òùÔ∏è As you can see, the chat history contains the modified user message with the instructions and the parsed LLM output. Since `replace` is set to `True`, the original message is replaced with the parsed message. \n",
    "\n",
    "üëá However, we can take a look at the original message in the `info` dictionary:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56f7e4c46b3b7a8d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here is a JSON object representing a Marvel hero:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Iron Man\",\n",
      "  \"age\": 40,\n",
      "  \"powers\": [\n",
      "    \"Superhuman strength\",\n",
      "    \"Flight\",\n",
      "    \"Energy blasts\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Let's get the assistant message from the chat history (i.e. the LLM's response)\n",
    "assistant_message = chat[-1]\n",
    "\n",
    "# Get the info dictionary from the assistant message\n",
    "info = assistant_message.info \n",
    "\n",
    "# Get the original message from the info dictionary\n",
    "original_message = info[\"original\"]\n",
    "\n",
    "print(original_message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T18:18:53.633972355Z",
     "start_time": "2023-08-06T18:18:53.632510955Z"
    }
   },
   "id": "226f8b81e51583e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ô∏èÔ∏èÔ∏è‚òùÔ∏è Here the LLM's response may also have some text above and below the json block (**Note:** This can vary from run to run). Any surrounding text was pruned by the `JSONDirective` when parsing the LLM's response (because `prune` was set to `True`) which is why the message in the chat history only contains the json block."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "951628f63f4f8564"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! üéâ You just created your first chat with a `JSONDirective`! üöÄ \n",
    "\n",
    "In this notebook, we learned how to embed a user message with a `JSONDirective` and how to parse the LLM's response. We also learned how to create a chat with a `JSONDirective` and how to access the original message in the chat history. \n",
    "\n",
    "**Note:** In this example we used OpenAI's LLM. However, the `JSONDirective` can be used with any LLM. You can also adapt the instructions of a directive via the `instructions` argument. \n",
    "\n",
    "If you have any question to hesitate to reach out to us on [Discord](https://discord.gg/wMHVAaqh).\n",
    "If you've found a bug, a spelling mistake or suggestions on what could be improved, please open an issue or a pull request on [GitHub](https://github.com/cereisen/lobsang).\n",
    "\n",
    "See you in the next part! üëã We hope we got you hooked üé£ "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42496dbfb5ec675"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
