{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:10.546104763Z",
     "start_time": "2023-08-04T14:33:09.605860709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lobsang in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: openai in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (0.27.4)\r\n",
      "Requirement already satisfied: python-dotenv in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (0.21.0)\r\n",
      "Requirement already satisfied: requests>=2.20 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from openai) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from openai) (4.65.0)\r\n",
      "Requirement already satisfied: aiohttp in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from openai) (3.8.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.7.22)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (22.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (6.0.2)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (1.8.1)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (1.2.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lobsang openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basics\n",
    "\n",
    "In this notebook, we'll cover the basics of lobsang. As a LLM we use OpenAI's \"gpt-3.5-turbo\" model, so you'll need an [OpenAI API key](https://platform.openai.com/account/api-keys) to run this notebook (set it in the cell below).\n",
    "This is not a free service, but all examples in this notebook should cost less than $0.01 to run. Nevertheless, you should set a [usage limit](https://platform.openai.com/account/billing/limits) to avoid any surprises."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4293c40b629b5af1"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "\"All set! 🎉 Let's get started! 🚀 OPENAI_API_KEY=sk-d45JqOfunEgN...\""
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import ChatCompletion\n",
    "\n",
    "from lobsang import Chat, LLM\n",
    "from lobsang.messages import SystemMessage, UserMessage, AssistantMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load OpenAI API key from .env file (please update .env file with your own API key)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert OPENAI_API_KEY, \"Please set OPENAI_API_KEY in .env file\"\n",
    "\n",
    "f\"All set! 🎉 Let's get started! 🚀 OPENAI_API_KEY={OPENAI_API_KEY[:15]}...\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:10.551837136Z",
     "start_time": "2023-08-04T14:33:10.548380917Z"
    }
   },
   "id": "d435b5427783c0ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Message Class\n",
    "\n",
    "Before we get started, let's take a look at the `Message` class. This is the abstract base class for all messages in lobsang. It has three subclasses: `SystemMessage`, `UserMessage`, and `AssistantMessage`.\n",
    "\n",
    "1. `SystemMessage` is a special case and is used to set the model's behavior (There should only be one `SystemMessage` per `Chat` instance at the beginning of the conversation.)\n",
    "1. `UserMessage` is used for messages that are created by the user/developer.\n",
    "1. `AssistantMessage` is used for messages that are sent generated by the llm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f29e3c0077697d70"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__repr__\n",
      "SystemMessage(role=system, text=You are a helpful assistant., info={})\n",
      "UserMessage(role=user, text=Hello, my name is Bark Twain., info={})\n",
      "AssistantMessage(role=assistant, text=Hello Bark, I'm Lobsang., info={})\n",
      "\n",
      "__str__\n",
      "SYSTEM: You are a helpful assistant.\n",
      "USER: Hello, my name is Bark Twain.\n",
      "ASSISTANT: Hello Bark, I'm Lobsang.\n"
     ]
    }
   ],
   "source": [
    "system_message = SystemMessage(\"You are a helpful assistant.\")\n",
    "user_message = UserMessage(\"Hello, my name is Bark Twain.\")\n",
    "assistant_message = AssistantMessage(\"Hello Bark, I'm Lobsang.\")\n",
    "\n",
    "messages = [system_message, user_message, assistant_message]\n",
    "\n",
    "# The message class implements the __repr__ contract/interface that returns a string representation of the message (useful for debugging)\n",
    "print(\"__repr__\")\n",
    "print(*map(repr, messages), sep=\"\\n\") # 💡 '*...' unpacks the list into individual arguments (i.e. print(*[1,2,3]) is the same as print(1,2,3)) \n",
    "    \n",
    "# And the __str__ contract/interface as well that returns a chat-formatted string representation of the message (<ROLE>: <TEXT>)\n",
    "print(\"\\n__str__\")\n",
    "print(*messages, sep=\"\\n\") # 💡 print automatically calls __str__ on objects"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:10.599400867Z",
     "start_time": "2023-08-04T14:33:10.551911045Z"
    }
   },
   "id": "cfbb7d2b6aee2be7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ As you can see, each message has a `role`, `text` and `info` attribute. \n",
    "1. `role` is the role of the message sender (i.e. \"user\", \"assistant\", or \"system\")\n",
    "2. `text` is the text of the message\n",
    "3. `info` is a dictionary that can be used to store additional information about the message"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b93acd4caf45d01b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLM Wrapper\n",
    "\n",
    "For experiments and testing, we also provide a FakeLLM which can be imported via `from lobsang.llms import FakeLLM`. However, in this notebook we'll use OpenAI's API. So before we can start chatting, we need to wrap the openai api to make it compatible with lobsang. This can be done with lobsang's `LLM` class as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fbb4516af539d88"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class OpenAI(LLM):\n",
    "    \"\"\"Wrapper for OpenAI API\"\"\"\n",
    "    def __init__(self):\n",
    "        self.api_key = OPENAI_API_KEY\n",
    "        self.model = \"gpt-3.5-turbo\"\n",
    "        \n",
    "    def chat(self, messages) -> (str, dict):\n",
    "        \"\"\"\n",
    "        Chat with OpenAI API\n",
    "        \n",
    "        Takes a list of messages and converts them to the format required by the OpenAI API.\n",
    "        Then, calls the API and retrieves the response text to return it to the caller.\n",
    "        \"\"\"\n",
    "        # Prepare messages\n",
    "        messages = [{'role': m.role, 'content': m.text} for m in messages]\n",
    "        \n",
    "        # Call OpenAI API\n",
    "        res = ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            api_key=self.api_key,\n",
    "            messages=messages,\n",
    "        )\n",
    "        \n",
    "        # Return text response\n",
    "        return res.choices[0].message.content, {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:10.600322115Z",
     "start_time": "2023-08-04T14:33:10.599099449Z"
    }
   },
   "id": "9c9962d0a47c7842"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: In this example, we return an empty dictionary as the second return value. However, all keys added to the dictionary will end up in the `info` attribute of the corresponding `AssistantMessage` that is created by the chat instance from the response text. This can be used to store additional information about the message. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15dc69bf8e02d149"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat Instance\n",
    "\n",
    "Now, let's bring all the pieces together and create a `Chat` instance. In this example, we'll call chat directly with a string, i.e. `chat(\"Ping\")`. \n",
    "This is equivalent to `chat.run([UserMessage(\"Ping\")])[1]` or `chat.run([\"Ping\"])[1]` (we'll get to the `run` method in a bit), but since this is verbose we provide a shortcut for this common use case with the `__call__` method, i.e. `chat(\"Ping\")`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d424b6672d5ade3"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "AssistantMessage(role=assistant, text=Pong! How can I assist you today?, info={'directive': TextDirective, 'query': UserMessage(role=user, text=Ping., info={})})"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first create a chat instance with the LLM wrapper from above\n",
    "chat = Chat(llm=OpenAI())  # 👈 Note that we don't pass a system message here, so the model will use its default behavior \"You are a helpful assistant.\"\n",
    "\n",
    "# And we are ready to go! 🚶 Let's chat! 🗣\n",
    "res = chat(\"Ping.\")\n",
    "\n",
    "# Let's see what we got back\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:11.436319909Z",
     "start_time": "2023-08-04T14:33:10.599240445Z"
    }
   },
   "id": "97382db5b7752f2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ As you can see, the response is a single `AssistantMessage` instance. It has a `role`, `text` and `info` attribute. The `role` is \"assistant\" as we'd suspect. The `text` is the response from the llm. And the `info` is a dictionary with additional information about the message, for example to help with debugging.\n",
    "\n",
    "Alright, in the next example, we'll initialize the chat instance with a `SystemMessage` to set the model's behavior. \n",
    "Let's make it a fancy wizard 🧙‍♂️ that speaks in a medieval style:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cdaffdf56456cc3"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, good sir/madam, thou hast asked a most difficult question! As a fancy wizard, mine heart doth hold affection for many a spell. Yet, if I must choose but one, I would humbly declare that the enchantment of Teleportation doth hold the highest place of reverence in mine arcane repertoire. 'Tis a spell that doth allow me to traverse great distances in the blink of an eye, traversing the realms with but a whispered incantation. Verily, it is a spell that grants me the power to appear in distant lands, beyond the limits of time and space, and to witness the wonders that lie there. Thus, Teleportation is a spell that fills me with joy and wonder at the possibilities that magic doth offer.\n"
     ]
    }
   ],
   "source": [
    "# First, we need to write a system message that sets the model's behavior\n",
    "system_message = \"You are a fancy wizard and talk in a medieval style.\"\n",
    "\n",
    "# Then, we create a chat instance again with the LLM wrapper from above and pass the system message\n",
    "chat = Chat(llm=OpenAI(), system_message=system_message)\n",
    "\n",
    "# And we are ready to go! 🚶 Let's chat! 🗣\n",
    "res = chat(\"What is you favourite spell?\")\n",
    "\n",
    "print(res.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:18.274030228Z",
     "start_time": "2023-08-04T14:33:11.436706809Z"
    }
   },
   "id": "4a9e0b43d1388f94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ Awesome! 🎉 In a couple of lines of code, we've created a fancy wizard ‍🧙‍♂️\n",
    "\n",
    "Now, it's time to let you in on a little secret 👀 The `Chat` class actually inherits from `list` 🤯 and contains the chat history.\n",
    "This means, we can do all the usual list operations on it.\n",
    "\n",
    "> **Note:** Please note that not all list operations are supported. For example, sorting is not supported. If you are missing a list operation, please open an issue on GitHub. Thanks! 🙏\n",
    "\n",
    "👇 Now, let's take a look at the chat history:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a43f8a692979fac"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is you favourite spell?\n",
      "ASSISTANT: Ah, good sir/madam, thou hast asked a most difficult question! As a fancy wizard, mine heart doth hold affection for many a spell. Yet, if I must choose but one, I would humbly declare that the enchantment of Teleportation doth hold the highest place of reverence in mine arcane repertoire. 'Tis a spell that doth allow me to traverse great distances in the blink of an eye, traversing the realms with but a whispered incantation. Verily, it is a spell that grants me the power to appear in distant lands, beyond the limits of time and space, and to witness the wonders that lie there. Thus, Teleportation is a spell that fills me with joy and wonder at the possibilities that magic doth offer.\n"
     ]
    }
   ],
   "source": [
    "print(chat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:18.302082821Z",
     "start_time": "2023-08-04T14:33:18.243028094Z"
    }
   },
   "id": "730774f9dea7820"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ As you can see, the chat history contains our previous message and the response from the model.\n",
    "\n",
    "👇 The following shows a selection of supported list operations:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ec8f71295f6c41b"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is you favourite spell?\n",
      "ASSISTANT: Ah, good sir/madam, thou hast asked a most difficult question! As a fancy wizard, mine heart doth hold affection for many a spell. Yet, if I must choose but one, I would humbly declare that the enchantment of Teleportation doth hold the highest place of reverence in mine arcane repertoire. 'Tis a spell that doth allow me to traverse great distances in the blink of an eye, traversing the realms with but a whispered incantation. Verily, it is a spell that grants me the power to appear in distant lands, beyond the limits of time and space, and to witness the wonders that lie there. Thus, Teleportation is a spell that fills me with joy and wonder at the possibilities that magic doth offer.\n"
     ]
    }
   ],
   "source": [
    "# We can iterate over the chat history\n",
    "for message in chat:\n",
    "    print(message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:18.302257220Z",
     "start_time": "2023-08-04T14:33:18.243143081Z"
    }
   },
   "id": "6a031f836fa44b19"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also get the length of the chat history\n",
    "len(chat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:18.302429114Z",
     "start_time": "2023-08-04T14:33:18.243204356Z"
    }
   },
   "id": "7506d83894599dff"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "UserMessage(role=user, text=What is you favourite spell?, info={})"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can index into it (or slice it, i.e chat[i:j])\n",
    "chat[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:18.302591992Z",
     "start_time": "2023-08-04T14:33:18.243278466Z"
    }
   },
   "id": "e9f94c9839034b92"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: I'm a new message\n",
      "ASSISTANT: Ah, good sir/madam, thou hast asked a most difficult question! As a fancy wizard, mine heart doth hold affection for many a spell. Yet, if I must choose but one, I would humbly declare that the enchantment of Teleportation doth hold the highest place of reverence in mine arcane repertoire. 'Tis a spell that doth allow me to traverse great distances in the blink of an eye, traversing the realms with but a whispered incantation. Verily, it is a spell that grants me the power to appear in distant lands, beyond the limits of time and space, and to witness the wonders that lie there. Thus, Teleportation is a spell that fills me with joy and wonder at the possibilities that magic doth offer.\n"
     ]
    }
   ],
   "source": [
    "# and replace messages\n",
    "chat[0] = UserMessage(\"I'm a new message\")\n",
    "print(chat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:18.302730523Z",
     "start_time": "2023-08-04T14:33:18.243373866Z"
    }
   },
   "id": "9a76ed24d38fbe9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ Other operations are also supported, please try them out yourself. For example, append(), extend() and copy() are supported as well.\n",
    "\n",
    "👇 Now that we know how to call chat and how to access the chat history, we'll take a look at one more useful feature.\n",
    "Most of the time you want your message as well as the response automatically added to the chat history. So this is the default behavior of the `Chat` class. However, sometimes you might want to call the model without adding the message to the chat history. For example, if you want to explore variations of a message. For this purpose you can set the `append` flag/parameter to `False` when calling chat: `chat(\"Ping\", append=False)`. This will return the response message without adding anything to the chat history."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "639d7c169cce0d51"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The sum of 1+1 is 2.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "chat = Chat(llm=OpenAI())\n",
    "\n",
    "res = chat(\"What is 1+1?\", append=False)\n",
    "\n",
    "print(\"Response:\", res.text)  \n",
    "print(len(chat))  # 👈 Will be 0 as we didn't append the message"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:19.236018648Z",
     "start_time": "2023-08-04T14:33:18.243431184Z"
    }
   },
   "id": "7ec7adcb2dd5cd77"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is 1+1?\n",
      "ASSISTANT: 1+1 equals 2.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Now, let's append the message and check the length again\n",
    "chat(\"What is 1+1?\")\n",
    "print(chat)\n",
    "print(len(chat))  # 👈 Will be 1 as we appended the message"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:19.879085744Z",
     "start_time": "2023-08-04T14:33:19.235289753Z"
    }
   },
   "id": "16a2ca680d8195"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validate() Method\n",
    "\n",
    "Sometimes, it might be necessary to validate the chat history. For example, if you want to change the chat history manually. In this case, you can call `chat.validate()`. This will check if the chat history is valid and raise an error if it is not. Primarily, this will check if the chat history is consistent, i.e. if each message has a response and that the chat history starts with a `UserMessage` and ends with an `AssistantMessage` (otherwise the chat history would be incomplete / inconsistent). For more information and examples,  please refer to the [documentation](https://docs.convokit.cornell.edu/python/chat.html#convokit.chat.Chat.validate).\n",
    "\n",
    "**Note:** This is a very strict validation, it is therefore not used internally but, for example, can be useful for debugging. Also, invalid chat history can still be used with the `Chat` class. However, it might lead to unexpected behavior.\n",
    "\n",
    "Let's check out an example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c126cb6a919b777"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Chat is missing an assistant message at the end.\n"
     ]
    }
   ],
   "source": [
    "# New chat instance\n",
    "chat = Chat(llm=OpenAI())\n",
    "\n",
    "# Let's make the chat history invalid by extending it\n",
    "invalid = [UserMessage(\"Ping\"), AssistantMessage(\"Pong\"), UserMessage(\"Ping\")] # 👈 Missing response for last message\n",
    "chat.extend(invalid)\n",
    "\n",
    "# Validate the chat history\n",
    "try:\n",
    "    chat.validate()  # 👈 Will raise an error\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:19.890879819Z",
     "start_time": "2023-08-04T14:33:19.880753830Z"
    }
   },
   "id": "91a474621d792966"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Ping\n",
      "ASSISTANT: Pong\n",
      "USER: Ping\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the current chat history (before we fix it)\n",
    "print(chat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:19.891030824Z",
     "start_time": "2023-08-04T14:33:19.883303679Z"
    }
   },
   "id": "3696f42af8c4079e"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Ping\n",
      "ASSISTANT: Pong\n",
      "USER: Ping\n",
      "ASSISTANT: Pong\n"
     ]
    }
   ],
   "source": [
    "# Now, let's fix the chat history\n",
    "chat.append(AssistantMessage(\"Pong\"))\n",
    "\n",
    "# and re-validate it\n",
    "chat.validate()  # Finger's crossed 🤞\n",
    "\n",
    "# Let's take a look at the chat history again\n",
    "print(chat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:19.891156541Z",
     "start_time": "2023-08-04T14:33:19.885304022Z"
    }
   },
   "id": "bd167d1e41e5aace"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run() Method\n",
    "\n",
    "In addition, to passing a single message to the `Chat` class, you can also pass a list of messages. For each unanswered message (we'll explain what this means in a second), the `Chat` class will call the model and append the message as well as the response to the conversation which is returned in the end. If the `append` flag/parameter is set to `True` (default), the messages and responses will also be appended to the chat history. \n",
    "\n",
    "For this, we can use the `run` method which takes a list of messages and returns the conversation with the messages and responses from the model. Let's take a look:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f62ee3c1cfd65f7d"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is 1+1?\n",
      "ASSISTANT: 2\n",
      "USER: What is 2+2?\n",
      "ASSISTANT: 4\n",
      "USER: What is 3+3?\n",
      "ASSISTANT: 6\n",
      "USER: What is 4+4?\n",
      "ASSISTANT: 8\n"
     ]
    }
   ],
   "source": [
    "chat = Chat(llm=OpenAI())\n",
    "\n",
    "# Let's create a list of messages\n",
    "# The first user message is already answered (i.e. it's an example on how the model should respond)\n",
    "# The other user messages are unanswered (i.e. they have no corresponding assistant message) and will be answered by the model one-by-one\n",
    "messages = [UserMessage(\"What is 1+1?\"), AssistantMessage(\"2\"), UserMessage(\"What is 2+2?\"), UserMessage(\"What is 3+3?\"), UserMessage(\"What is 4+4?\")]\n",
    "\n",
    "conversation = chat.run(messages)\n",
    "print(*conversation, sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:21.331545838Z",
     "start_time": "2023-08-04T14:33:19.887621613Z"
    }
   },
   "id": "4cb9752c6b437944"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ As you can see the conversation is a list of messages. The first message is the first user message, followed by our predefined response from the model. The second message is the second user message, followed by the response from the model and so on. \n",
    "\n",
    "👇 If you do not want the example to be part of the conversation you can do the following:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ac48862555a32d9"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is 2+2?\n",
      "ASSISTANT: 4\n",
      "USER: What is 3+3?\n",
      "ASSISTANT: 6\n",
      "USER: What is 4+4?\n",
      "ASSISTANT: 8\n"
     ]
    }
   ],
   "source": [
    "example = [UserMessage(\"What is 1+1?\"), AssistantMessage(\"2\")]\n",
    "chat = Chat(example, llm=OpenAI())  # 👈 Pass the example to the Chat as initial chat history\n",
    "\n",
    "messages = messages[2:]  # 👈 Remove the example from the list of messages\n",
    "\n",
    "conversation = chat.run(messages)  # 👈 This will also append the messages and responses to the chat history (as append=True by default)\n",
    "print(*conversation, sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:22.578330622Z",
     "start_time": "2023-08-04T14:33:21.332961498Z"
    }
   },
   "id": "40b34fe4ceeb0f96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ Now, the example is not part of the conversation anymore. \n",
    "\n",
    "👇 Note that it is part of the chat history though:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b65e5e92a42750a4"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is 1+1?\n",
      "ASSISTANT: 2\n",
      "USER: What is 2+2?\n",
      "ASSISTANT: 4\n",
      "USER: What is 3+3?\n",
      "ASSISTANT: 6\n",
      "USER: What is 4+4?\n",
      "ASSISTANT: 8\n"
     ]
    }
   ],
   "source": [
    "print(chat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T14:33:22.583577348Z",
     "start_time": "2023-08-04T14:33:22.578805628Z"
    }
   },
   "id": "82611403a831a802"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "🤓 This concludes the first part of the tutorial. You should now be able to use the `Chat` class to interact with the model. \n",
    "In the next part, we'll discuss the concept of directives, which will send you through the roof 🚀 (Metaphorically speaking of course 😅).\n",
    "\n",
    "If you have any question to hesitate to reach out to us on [Discord](https://discord.gg/wMHVAaqh).\n",
    "If you've found a bug, a spelling mistake or suggestions on what could be improved, please open an issue or a pull request on [GitHub](https://github.com/cereisen/lobsang).\n",
    "\n",
    "See you in the next part! 👋 We hope we got you hooked 🎣 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d6aabe8a381b03"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
