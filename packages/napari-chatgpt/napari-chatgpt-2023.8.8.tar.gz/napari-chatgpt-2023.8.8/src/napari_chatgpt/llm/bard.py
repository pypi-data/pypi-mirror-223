import asyncio
import re
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import Dict, List, Optional

from Bard import Chatbot
from langchain.callbacks.manager import (
    CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun,
)
from langchain.llms.base import LLM
from langchain.llms.utils import enforce_stop_tokens
from pydantic import Field
from pydantic import root_validator

_aysync_bard_thread_pool = ThreadPoolExecutor()


class ChatBard(LLM):
    r"""Wrapper around Google's large language model Bard.

    """

    streaming: bool = False
    """Whether to stream the results or not."""

    bard_token: str = Field(default=None)

    bard: Chatbot = None

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that the python package exists in the environment."""
        pass
        return values

    @property
    def _llm_type(self) -> str:
        """Return the type of llm."""
        return "google-bard"

    def _call(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[CallbackManagerForLLMRun] = None,
    ) -> str:
        r"""Call out to Bard's generate method.

        Args:
            prompt: The prompt to pass into the model.
            stop: A list of strings to stop generation when encountered.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                prompt = "Once upon a time, "
                response = model(prompt, n_predict=55)
        """

        text_callback = None
        if run_manager:
            text_callback = partial(run_manager.on_llm_new_token,
                                    verbose=self.verbose)
        text = ""
        for token in self._bard(prompt):
            if text_callback:
                text_callback(token)
            text += token
        if stop is not None:
            text = enforce_stop_tokens(text, stop)
        return text

    async def _acall(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
    ) -> str:
        """Run the LLM on the given prompt and input."""
        result = await asyncio.get_running_loop().run_in_executor(
            _aysync_bard_thread_pool,
            self._call,
            prompt,
            stop,
            run_manager
        )
        return result

    def _bard(self,
              prompt: str):

        if not self.bard:
            self.bard = Chatbot(self.bard_token)

        response = self.bard.ask(prompt)
        text = response['content']
        words = re.split(r"(\s+)", text)
        for word in words:
            yield word
