{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your own neos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey, multivariate_normal\n",
    "import pyhf\n",
    "from typing import Callable, Any, Generator, Iterable\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import neos\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "pyhf.set_backend(\"jax\")\n",
    "\n",
    "# matplotlib settings\n",
    "plt.rc(\"figure\", figsize=(6, 2), dpi=150, facecolor=\"w\")\n",
    "\n",
    "Array = jnp.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "`neos` tackles the problem of learning optimal summary statistics from data. The workflow is explained in more detail [in our paper]() -- this notebook is designed to get you up-and-running with how that workflow looks in code, including how to do this for your own use-case! (if you want to do that for real, please reach out -- we'd love to help you!)\n",
    "\n",
    "To construct this kind of workflow, you need to specify three things:\n",
    "- How to construct your summary statistic from data\n",
    "- How to build your likelihood function from that summary statistic\n",
    "- A choice of metric relating to how you deem your analysis to be \"optimal\"\n",
    "\n",
    "The second bullet in-particular is very important; while we've abstracted away all the technical detail, `neos` is still not a drop-in loss function in the typical sense, since it requires *detailed information from the analyser on how to build the likelihood*. No longer do we have a problem-agnostic notion of \"signal\" and \"background\"; instead, we're wanting to optimise our specific problem, so we need to provide specific information. That is how `neos` is \"systematic-aware\" -- it's a technique that explicitly takes into account how your systematics are modelled. But to do this, we of course need to model them in the first place!\n",
    "\n",
    "Here, we're going to re-implement the example from the paper with 2-dimensional Gaussian blobs making up our data. We have a nominal estimate for signal and background data, and we also provide \"up\" and \"down\" variations that correspond to moving the mean of the background blob. The helper function to generate this dataset is below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation helper function + visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(\n",
    "    rng: int = 0,\n",
    "    num_points: int = 10000,\n",
    "    sig_mean: tuple[float, float] = (-1, 1),\n",
    "    bup_mean: tuple[float, float] = (2.5, 2),\n",
    "    bdown_mean: tuple[float, float] = (-2.5, -1.5),\n",
    "    b_mean: tuple[float, float] = (1, -1),\n",
    ") -> tuple[Array, Array, Array, Array]:\n",
    "    sig = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(sig_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "    bkg_up = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(bup_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "    bkg_down = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(bdown_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "\n",
    "    bkg_nom = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(b_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "    return sig, bkg_nom, bkg_up, bkg_down\n",
    "\n",
    "\n",
    "data = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_space(\n",
    "    ax: Any,\n",
    "    data: Array,\n",
    "    params: dict[str, Any] | None = None,\n",
    "    nn: Callable | None = None,\n",
    "    bins: Array | None = None,\n",
    ") -> None:\n",
    "    if nn is not None:\n",
    "        network = params[\"nn_pars\"]\n",
    "        bins = params[\"bins\"] if \"bins\" in params else bins\n",
    "        g = jnp.mgrid[-10:10:101j, -10:10:101j]\n",
    "        levels = [0, *bins, 1]\n",
    "        ax.contourf(\n",
    "            g[0],\n",
    "            g[1],\n",
    "            nn(network, jnp.moveaxis(g, 0, -1)).reshape(101, 101, 1)[:, :, 0],\n",
    "            levels=levels,\n",
    "            cmap=\"binary\",\n",
    "        )\n",
    "        ax.contour(\n",
    "            g[0],\n",
    "            g[1],\n",
    "            nn(network, jnp.moveaxis(g, 0, -1)).reshape(101, 101, 1)[:, :, 0],\n",
    "            colors=\"w\",\n",
    "            levels=levels,\n",
    "        )\n",
    "    sig, bkg_nom, bkg_up, bkg_down = data\n",
    "    # should definitely not have to repeat this every time lmao\n",
    "    ax.scatter(sig[:, 0], sig[:, 1], alpha=0.3, c=\"C9\", label=\"signal\")\n",
    "    ax.scatter(\n",
    "        bkg_up[:, 0],\n",
    "        bkg_up[:, 1],\n",
    "        alpha=0.1,\n",
    "        c=\"orangered\",\n",
    "        marker=6,\n",
    "        label=\"bkg up\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        bkg_down[:, 0],\n",
    "        bkg_down[:, 1],\n",
    "        alpha=0.1,\n",
    "        c=\"gold\",\n",
    "        marker=7,\n",
    "        label=\"bkg down\",\n",
    "    )\n",
    "    ax.scatter(bkg_nom[:, 0], bkg_nom[:, 1], alpha=0.3, c=\"C1\", label=\"bkg\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[4, 4])\n",
    "plot_data_space(ax, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we need to specify how we construct our summary statistic. We will use a neural network for this, left unspecified until later. Since our likelihood modelling stage will later involve using histograms, we'll also construct a histogram of the output, then scale the yields in each batch for a more \"realistic\" analysis. This step is wrapped up in a simple convenience function called `neos.hists_from_nn`, which we'll use in a moment.\n",
    "\n",
    "After this comes the problem-specific part: the likelihood model construction. I mentioned before that we have \"up\" and \"down\" datasets -- this is a typical construct in HEP that we use to model the effect of physical parameters. We will then have three histograms for the background data: one for each of the nominal, up, and down samples. The HistFactory prescription for building likelihood functions then models the uncertainty between these by interpolating between their yields. Luckily, this is done under the hood for us in `pyhf` -- we just need to write a dictionary specifying the format of our histograms. This can be found below for our case, where we describe our uncertainty by forming a nuisance parameter `\"correlated_bkg_uncertainty\"` that modifies the shape of the histogram in a bin-correlated fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physicist's input: how to build your statistical model from histograms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we give a dict of histograms with keys \"sig\", \"bkg_nominal\", \"bkg_up\", \"bkg_down\".\n",
    "def model_from_hists(hists: dict[str, Array]) -> pyhf.Model:\n",
    "    \"\"\"How to make your HistFactory model from your histograms.\"\"\"\n",
    "    spec = {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",  # we only have one \"channel\" (data region)\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"signal\",\n",
    "                        \"data\": hists[\"sig\"],  # signal\n",
    "                        \"modifiers\": [\n",
    "                            {\n",
    "                                \"name\": \"mu\",\n",
    "                                \"type\": \"normfactor\",\n",
    "                                \"data\": None,\n",
    "                            },  # our signal strength modifier (parameter of interest)\n",
    "                        ],\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        \"data\": hists[\"bkg_nominal\"],  # background\n",
    "                        \"modifiers\": [\n",
    "                            {\n",
    "                                \"name\": \"correlated_bkg_uncertainty\",\n",
    "                                \"type\": \"histosys\",\n",
    "                                \"data\": {\n",
    "                                    \"hi_data\": hists[\"bkg_up\"],  # up sample\n",
    "                                    \"lo_data\": hists[\"bkg_down\"],  # down sample\n",
    "                                },\n",
    "                            },\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "    return pyhf.Model(spec, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to form our metric of choice from our likelihood model. Here, we have a variety to choose from: uncertainty on the signal strength modifier (cross-section), discovery significance, CLs, even the classic binary cross-entropy (that doesn't use the likelihood at all). Instead of overloading the notebook with losses, I've baked them all into `neos.loss_from_model`, which takes in a `pyhf` model and a string with the metric you want to calculate.\n",
    "\n",
    "With these ingredients, we just need to compose them in one function, so we can take the gradient of that function with respect to our free parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pipeline by combining functions from the `neos` module with our model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    pars: dict[str, Array],\n",
    "    data: tuple[Array, ...],\n",
    "    nn: Callable,\n",
    "    loss: str,\n",
    "    bandwidth: float,\n",
    "    sample_names: Iterable[str],  # we're using a list of dict keys for bookkeeping!\n",
    "    scale_factors: dict[str, float],\n",
    "    bins: Array | None = None,  # in case you don't want to optimise binning\n",
    "    lumi: float = 10.0,  # overall scale factor\n",
    ") -> float:\n",
    "    # zip up our data arrays with the corresponding sample names\n",
    "    data_dct = {k: v for k, v in zip(sample_names, data)}\n",
    "\n",
    "    # if you want s/b discrimination, no need to do anything complex!\n",
    "    if loss.lower() in [\"bce\", \"binary cross-entropy\"]:\n",
    "        return neos.losses.bce(data=data_dct, pars=pars[\"nn_pars\"], nn=nn)\n",
    "\n",
    "    # use a neural network + differentiable histograms [bKDEs] to get the yields\n",
    "    hists = neos.hists_from_nn(\n",
    "        pars=pars[\"nn_pars\"],\n",
    "        nn=nn,\n",
    "        data=data_dct,\n",
    "        bandwidth=bandwidth,  # for the bKDEs\n",
    "        bins=jnp.array([0, *pars[\"bins\"], 1]) if \"bins\" in pars else bins,\n",
    "        scale_factors=scale_factors,\n",
    "        overall_scale=lumi,\n",
    "    )\n",
    "\n",
    "    # build our statistical model, and calculate the loss!\n",
    "    model = model_from_hists(hists)\n",
    "    return neos.loss_from_model(model, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is now mostly training boilerplate! I'll add titles, but should be mostly clear what's going on (ask if not!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise network architecture + params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.example_libraries import stax\n",
    "\n",
    "rng_state = 0  # random state\n",
    "\n",
    "# feel free to modify :)\n",
    "init_random_params, nn = stax.serial(\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(1),\n",
    "    stax.Sigmoid,\n",
    ")\n",
    "\n",
    "num_features = 2\n",
    "_, init = init_random_params(PRNGKey(rng_state), (-1, num_features))\n",
    "init_pars = dict(nn_pars=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define batching mechanism for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy.random as npr\n",
    "\n",
    "batch_size = 2000  # change me if you want!\n",
    "split = train_test_split(*data, random_state=rng_state)\n",
    "train, test = split[::2], split[1::2]\n",
    "\n",
    "\n",
    "def batches(training_data: Array, batch_size: int) -> Generator:\n",
    "    num_train = training_data[0].shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "    # batching mechanism, ripped from the JAX docs :)\n",
    "    def data_stream():\n",
    "        rng = npr.RandomState(rng_state)\n",
    "        while True:\n",
    "            perm = rng.permutation(num_train)\n",
    "            for i in range(num_batches):\n",
    "                batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "                yield [points[batch_idx] for points in train]\n",
    "\n",
    "    return data_stream()\n",
    "\n",
    "\n",
    "batch_iterator = batches(train, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxopt import OptaxSolver\n",
    "import optax\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "def run_optimisation(\n",
    "    num_steps: int,\n",
    "    objective: str,\n",
    "    test_objective: str,\n",
    "    lr: float,\n",
    "    bins: Array,\n",
    "    data_types: Iterable[str],\n",
    "    scale_factors: dict[str, float],\n",
    "    include_bins: bool = True,\n",
    ") -> tuple[Array, dict[str, list]]:\n",
    "    loss = partial(\n",
    "        pipeline,\n",
    "        nn=nn,\n",
    "        sample_names=data_types,\n",
    "        scale_factors=scales,\n",
    "    )\n",
    "\n",
    "    solver = OptaxSolver(loss, opt=optax.adam(lr), jit=True)\n",
    "\n",
    "    pyhf.set_backend(\"jax\", default=True)\n",
    "\n",
    "    if include_bins:\n",
    "        init_pars[\"bins\"] = bins[\n",
    "            1:-1\n",
    "        ]  # don't want to float endpoints [will account for kde spill]\n",
    "        state = solver.init_state(init_pars)\n",
    "    else:\n",
    "        if \"bins\" in init_pars:\n",
    "            del init_pars[\"bins\"]\n",
    "        state = solver.init_state(init_pars)\n",
    "\n",
    "    params = init_pars\n",
    "    best_params = init_pars\n",
    "    best_sig = 999\n",
    "    metrics = {k: [] for k in [\"cls\", \"discovery\", \"poi_uncert\"]}\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        print(f\"step {i}: loss={objective}\")\n",
    "        data = next(batch_iterator)\n",
    "        start = perf_counter()\n",
    "        params, state = solver.update(\n",
    "            params, state, bins=bins, data=data, loss=objective, bandwidth=9e-2\n",
    "        )\n",
    "        end = perf_counter()\n",
    "        print(f\"update took {end-start:.4g}s\")\n",
    "        if \"bins\" in params:\n",
    "            print(\"bin edges: [0 \", *[f\"{f:.3g}\" for f in params[\"bins\"]], \" 1]\")\n",
    "        for metric in metrics:\n",
    "            test_metric = loss(\n",
    "                params, bins=bins, data=test, loss=metric, bandwidth=1e-8\n",
    "            )\n",
    "            print(f\"{metric}={test_metric:.4g}\")\n",
    "            metrics[metric].append(test_metric)\n",
    "        if metrics[\"discovery\"][-1] < best_sig:\n",
    "            best_params = params\n",
    "            best_sig = metrics[\"discovery\"][-1]\n",
    "        print()\n",
    "    return best_params, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss=cls\n",
      "update took 4.361s\n",
      "bin edges: [0  0.251 0.501 0.749  1]\n",
      "cls=0.001816\n",
      "discovery=0.01036\n",
      "poi_uncert=0.4561\n",
      "\n",
      "step 1: loss=cls\n",
      "update took 2.582s\n",
      "bin edges: [0  0.252 0.5 0.749  1]\n",
      "cls=3.777e-06\n",
      "discovery=0.009022\n",
      "poi_uncert=0.4867\n",
      "\n",
      "step 2: loss=cls\n",
      "update took 2.487s\n",
      "bin edges: [0  0.252 0.5 0.748  1]\n",
      "cls=6.712e-07\n",
      "discovery=0.002232\n",
      "poi_uncert=0.4361\n",
      "\n",
      "step 3: loss=cls\n",
      "update took 2.701s\n",
      "bin edges: [0  0.253 0.499 0.748  1]\n",
      "cls=4.506e-07\n",
      "discovery=0.0009347\n",
      "poi_uncert=0.3329\n",
      "\n",
      "step 4: loss=cls\n",
      "update took 2.489s\n",
      "bin edges: [0  0.253 0.499 0.748  1]\n",
      "cls=5.716e-07\n",
      "discovery=0.0007648\n",
      "poi_uncert=0.2735\n",
      "\n",
      "step 5: loss=cls\n",
      "update took 2.529s\n",
      "bin edges: [0  0.254 0.498 0.747  1]\n",
      "cls=7.22e-07\n",
      "discovery=0.002349\n",
      "poi_uncert=0.2688\n",
      "\n",
      "step 6: loss=cls\n",
      "update took 2.532s\n",
      "bin edges: [0  0.254 0.498 0.747  1]\n",
      "cls=8.312e-07\n",
      "discovery=0.003934\n",
      "poi_uncert=0.3059\n",
      "\n",
      "step 7: loss=cls\n",
      "update took 2.415s\n",
      "bin edges: [0  0.254 0.498 0.747  1]\n",
      "cls=1.17e-06\n",
      "discovery=0.004561\n",
      "poi_uncert=0.3384\n",
      "\n",
      "step 8: loss=cls\n",
      "update took 2.633s\n",
      "bin edges: [0  0.254 0.498 0.747  1]\n",
      "cls=1.333e-06\n",
      "discovery=0.005545\n",
      "poi_uncert=0.3575\n",
      "\n",
      "step 9: loss=cls\n",
      "update took 2.543s\n",
      "bin edges: [0  0.255 0.497 0.747  1]\n",
      "cls=1.61e-06\n",
      "discovery=0.006541\n",
      "poi_uncert=0.3771\n",
      "\n",
      "step 10: loss=cls\n",
      "update took 2.542s\n",
      "bin edges: [0  0.255 0.497 0.747  1]\n",
      "cls=1.559e-06\n",
      "discovery=0.007125\n",
      "poi_uncert=0.3853\n",
      "\n",
      "step 11: loss=cls\n",
      "update took 2.488s\n",
      "bin edges: [0  0.255 0.497 0.746  1]\n",
      "cls=1.695e-06\n",
      "discovery=0.007666\n",
      "poi_uncert=0.3874\n",
      "\n",
      "step 12: loss=cls\n",
      "update took 2.582s\n",
      "bin edges: [0  0.255 0.497 0.746  1]\n",
      "cls=1.633e-06\n",
      "discovery=0.007484\n",
      "poi_uncert=0.3864\n",
      "\n",
      "step 13: loss=cls\n",
      "update took 2.433s\n",
      "bin edges: [0  0.255 0.497 0.746  1]\n",
      "cls=1.64e-06\n",
      "discovery=0.007148\n",
      "poi_uncert=0.3799\n",
      "\n",
      "step 14: loss=cls\n"
     ]
    }
   ],
   "source": [
    "include_bins = True  # simultaneously optimise binning and neural net!\n",
    "num_bins = 4\n",
    "bins = jnp.linspace(0, 1, num_bins + 1)  # keep in [0,1] if using sigmoid activation\n",
    "lr = 1e-3\n",
    "num_steps = 2  # increase me!\n",
    "# can choose from \"CLs\", \"discovery\", \"poi_uncert\" [approx. uncert. on mu], \"bce\" [classifier]\n",
    "objective = \"cls\"  # train wrt CLs...\n",
    "test_metric = \"discovery\"  # ...then select model with best discovery p-value\n",
    "\n",
    "# the same keys you used in the model building step [model_from_hists]\n",
    "data_types = [\"sig\", \"bkg_nominal\", \"bkg_up\", \"bkg_down\"]\n",
    "scales = {k: 2.0 if k == \"sig\" else 10.0 for k in data_types}\n",
    "\n",
    "\n",
    "best_params, metrics = run_optimisation(\n",
    "    num_steps, objective, test_metric, lr, bins, data_types, scales, include_bins\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random visualisations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_grid = (jnp.array(list(range(1, num_steps + 1))) * batch_size) / train[0].shape[0]\n",
    "for k, v in metrics.items():\n",
    "    if k != \"generalised_variance\":\n",
    "        plt.plot(epoch_grid, v, label=k)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"metric\")\n",
    "plt.savefig(\"float.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields = neos.hists_from_nn(\n",
    "    best_params[\"nn_pars\"],\n",
    "    {k: v for k, v in zip(data_types, test)},\n",
    "    nn,\n",
    "    bandwidth=1e-8,\n",
    "    scale_factors={k: 2.0 if k == \"sig\" else 10.0 for k in data_types},\n",
    "    bins=jnp.array([0, *best_params[\"bins\"], 1]),\n",
    ")\n",
    "binning = best_params[\"bins\"]\n",
    "\n",
    "for c, (l, a) in zip(\n",
    "    [\"C0\", \"C1\", \"C2\", \"C3\"], zip(yields, jnp.array(list(yields.values())))\n",
    "):\n",
    "    plt.bar(range(len(a)), a, label=l, alpha=0.4, fill=None, edgecolor=c, linewidth=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network contours (only relevant for the blobs problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_data_space(ax, test, best_params, nn=nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More hacky way of doing the same thing\n",
    "The above pipeline is actually limited by the fact that we've only got `pyhf` to construct models differentiably for this particular systematic type -- there are currently some technical roadblocks to doing it for others.\n",
    "\n",
    "One way to circumvent this is to just hack the model structure itself, because all we care about is the likelhood, and that uses class attributes to calculate it. We'll have to make a copy of the model each time so we don't mutate the global state of the program (`jax` hates when you do this), but it's possible, albeit a bit cumbersome to write. \n",
    "\n",
    "(apologies in advance if you read the code -- let's discuss if you want to do this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def hacked_pipeline(\n",
    "    pars: dict[str, Array],\n",
    "    data: tuple[Array, ...],\n",
    "    nn: Callable,\n",
    "    loss: str | Callable,\n",
    "    bandwidth: float,\n",
    "    sample_names: Iterable,\n",
    "    skeleton_model: pyhf.Model,  # new argument: dummy model with random yields\n",
    "    scale_factors: dict[str, float],\n",
    "    bins: Array | None = None,\n",
    "    lumi: float = 10.0,\n",
    ") -> float:\n",
    "    data_dct = {k: v for k, v in zip(sample_names, data)}\n",
    "    if loss.lower() in [\"bce\", \"binary cross-entropy\"]:\n",
    "        return neos.losses.bce(data=data_dct, pars=pars[\"nn_pars\"], nn=nn)\n",
    "    hists = neos.hists_from_nn(\n",
    "        pars=pars[\"nn_pars\"],\n",
    "        nn=nn,\n",
    "        data=data_dct,\n",
    "        bandwidth=bandwidth,\n",
    "        bins=jnp.array([0, *pars[\"bins\"], 1]) if \"bins\" in pars else bins,\n",
    "        scale_factors=scale_factors,\n",
    "        overall_scale=lumi,\n",
    "    )\n",
    "    # NB: below is a way to hack your model with yield information so you don't have\n",
    "    # to go through all the pyhf boilerplate when taking gradients.\n",
    "    # This could be the more realistic way to do this if you want to do it *now* for an\n",
    "    # arbitrary model that we haven't tested.\n",
    "    # to start: make a copy of a dummy model with the same spec as your real model\n",
    "    model = deepcopy(skeleton_model)\n",
    "\n",
    "    # the model yields are in two places in our case:\n",
    "    #   - the nominal sig + bkg yields\n",
    "    #   - the up/down/nominal templates for the systematic [histosys],\n",
    "    #     located in the interpolation settings\n",
    "    # => we find these attributes, and hack in our calculated values from the nn.\n",
    "    #\n",
    "    # start by getting the infromation on the interpolator type:\n",
    "    interpcode = model.main_model.modifiers_appliers[\"histosys\"].interpcode\n",
    "    interp_maker = getattr(pyhf.interpolators, interpcode)\n",
    "\n",
    "    # then we update the model in-place with the new nominal and template information:\n",
    "    def model_update_from_hists(hists):\n",
    "        nominal_rates = jnp.stack([hists[\"bkg_nominal\"], hists[\"sig\"]]).reshape(\n",
    "            model.main_model.nominal_rates.shape\n",
    "        )\n",
    "        histoset = [\n",
    "            [\n",
    "                [hists[\"bkg_down\"], hists[\"bkg_nominal\"], hists[\"bkg_up\"]],\n",
    "                [hists[\"sig\"]] * 3,\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        return nominal_rates, histoset\n",
    "\n",
    "    def update_model(new_nominal, new_histoset):\n",
    "        model.main_model.nominal_rates = new_nominal\n",
    "        interpolator = interp_maker(new_histoset)\n",
    "        model.main_model.modifiers_appliers[\"histosys\"].interpolator = interpolator\n",
    "\n",
    "    update_model(*model_update_from_hists(hists))\n",
    "\n",
    "    # back to normal code...\n",
    "    return neos.loss_from_model(model, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss=cls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phinate/code/relaxed/venv/lib/python3.9/site-packages/jax/_src/tree_util.py:188: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
      "  warnings.warn('jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update took 2.506s\n",
      "bin edges: [0  0.251 0.501 0.749  1]\n",
      "cls=0.003912\n",
      "discovery=0.009824\n",
      "poi_uncert=0.4526\n",
      "\n",
      "step 1: loss=cls\n",
      "update took 2.477s\n",
      "bin edges: [0  0.252 0.5 0.748  1]\n",
      "cls=9.274e-06\n",
      "discovery=0.008902\n",
      "poi_uncert=0.4859\n",
      "\n",
      "step 2: loss=cls\n",
      "update took 2.473s\n",
      "bin edges: [0  0.252 0.5 0.748  1]\n",
      "cls=1.447e-06\n",
      "discovery=0.002055\n",
      "poi_uncert=0.4358\n",
      "\n",
      "step 3: loss=cls\n",
      "update took 3.282s\n",
      "bin edges: [0  0.253 0.499 0.747  1]\n",
      "cls=9.776e-07\n",
      "discovery=0.0009045\n",
      "poi_uncert=0.3611\n",
      "\n",
      "step 4: loss=cls\n",
      "update took 2.485s\n",
      "bin edges: [0  0.253 0.499 0.747  1]\n",
      "cls=1.111e-06\n",
      "discovery=0.0006624\n",
      "poi_uncert=0.3007\n",
      "\n",
      "step 5: loss=cls\n",
      "update took 2.432s\n",
      "bin edges: [0  0.254 0.498 0.747  1]\n",
      "cls=1.268e-06\n",
      "discovery=0.0007909\n",
      "poi_uncert=0.2642\n",
      "\n",
      "step 6: loss=cls\n",
      "update took 2.436s\n",
      "bin edges: [0  0.254 0.498 0.747  1]\n",
      "cls=1.432e-06\n",
      "discovery=0.002224\n",
      "poi_uncert=0.27\n",
      "\n",
      "step 7: loss=cls\n",
      "update took 3.096s\n",
      "bin edges: [0  0.254 0.498 0.746  1]\n",
      "cls=1.807e-06\n",
      "discovery=0.002976\n",
      "poi_uncert=0.2939\n",
      "\n",
      "step 8: loss=cls\n",
      "update took 2.432s\n",
      "bin edges: [0  0.254 0.498 0.746  1]\n",
      "cls=2.518e-06\n",
      "discovery=0.004231\n",
      "poi_uncert=0.3203\n",
      "\n",
      "step 9: loss=cls\n",
      "update took 2.437s\n",
      "bin edges: [0  0.255 0.497 0.746  1]\n",
      "cls=2.697e-06\n",
      "discovery=0.00392\n",
      "poi_uncert=0.3322\n",
      "\n",
      "step 10: loss=cls\n",
      "update took 2.389s\n",
      "bin edges: [0  0.255 0.497 0.746  1]\n",
      "cls=2.759e-06\n",
      "discovery=0.004981\n",
      "poi_uncert=0.3446\n",
      "\n",
      "step 11: loss=cls\n",
      "update took 2.303s\n",
      "bin edges: [0  0.255 0.497 0.745  1]\n",
      "cls=2.876e-06\n",
      "discovery=0.005473\n",
      "poi_uncert=0.3513\n",
      "\n",
      "step 12: loss=cls\n",
      "update took 3.467s\n",
      "bin edges: [0  0.255 0.497 0.745  1]\n",
      "cls=3.035e-06\n",
      "discovery=0.005761\n",
      "poi_uncert=0.3573\n",
      "\n",
      "step 13: loss=cls\n",
      "update took 2.399s\n",
      "bin edges: [0  0.255 0.497 0.745  1]\n",
      "cls=2.904e-06\n",
      "discovery=0.005735\n",
      "poi_uncert=0.3544\n",
      "\n",
      "step 14: loss=cls\n",
      "update took 2.423s\n",
      "bin edges: [0  0.255 0.497 0.745  1]\n",
      "cls=2.958e-06\n",
      "discovery=0.005783\n",
      "poi_uncert=0.3499\n",
      "\n",
      "step 15: loss=cls\n",
      "update took 2.496s\n",
      "bin edges: [0  0.255 0.497 0.745  1]\n",
      "cls=2.916e-06\n",
      "discovery=0.005053\n",
      "poi_uncert=0.3452\n",
      "\n",
      "step 16: loss=cls\n",
      "update took 2.474s\n",
      "bin edges: [0  0.255 0.496 0.745  1]\n",
      "cls=2.846e-06\n",
      "discovery=0.004658\n",
      "poi_uncert=0.3396\n",
      "\n",
      "step 17: loss=cls\n",
      "update took 2.428s\n",
      "bin edges: [0  0.256 0.496 0.744  1]\n",
      "cls=2.679e-06\n",
      "discovery=0.004739\n",
      "poi_uncert=0.3335\n",
      "\n",
      "step 18: loss=cls\n",
      "update took 2.429s\n",
      "bin edges: [0  0.256 0.496 0.744  1]\n",
      "cls=2.698e-06\n",
      "discovery=0.00402\n",
      "poi_uncert=0.3213\n",
      "\n",
      "step 19: loss=cls\n",
      "update took 2.283s\n",
      "bin edges: [0  0.256 0.496 0.744  1]\n",
      "cls=2.693e-06\n",
      "discovery=0.003039\n",
      "poi_uncert=0.309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from jaxopt import OptaxSolver\n",
    "import optax\n",
    "from time import perf_counter\n",
    "\n",
    "include_bins = True\n",
    "num_bins = 4\n",
    "bins = jnp.linspace(0, 1, num_bins + 1)  # keep in [0,1] if using sigmoid activation\n",
    "lr = 1e-3\n",
    "num_steps = 20\n",
    "# can choose from \"CLs\", \"discovery\", \"poi_uncert\" [approx. uncert. on mu], \"bce\" [classifier]\n",
    "objective = \"cls\"\n",
    "keep_metric = \"discovery\"\n",
    "\n",
    "# the same keys you used in the model building step [model_from_hists]\n",
    "data_types = [\"sig\", \"bkg_nominal\", \"bkg_up\", \"bkg_down\"]\n",
    "scales = {k: 2.0 if k == \"sig\" else 10.0 for k in data_types}\n",
    "loss = partial(\n",
    "    hacked_pipeline,\n",
    "    nn=nn,\n",
    "    sample_names=data_types,\n",
    "    scale_factors=scales,\n",
    "    skeleton_model=model_from_hists(\n",
    "        {k: v for k, v in zip(data_types, [jnp.ones(num_bins)] * 4)}\n",
    "    ),\n",
    ")\n",
    "\n",
    "jax.jit(loss, static_argnames=(\"loss\"))\n",
    "\n",
    "solver = OptaxSolver(loss, opt=optax.adam(lr), jit=True)\n",
    "\n",
    "pyhf.set_backend(\"jax\", default=True)\n",
    "\n",
    "if include_bins:\n",
    "    init_pars[\"bins\"] = bins[\n",
    "        1:-1\n",
    "    ]  # don't want to float endpoints [will account for kde spill]\n",
    "    state = solver.init_state(init_pars)\n",
    "else:\n",
    "    if \"bins\" in init_pars:\n",
    "        del init_pars[\"bins\"]\n",
    "    state = solver.init_state(init_pars)\n",
    "\n",
    "params = init_pars\n",
    "best_params = init_pars\n",
    "best_sig = 999\n",
    "metrics = {k: [] for k in [\"cls\", \"discovery\", \"poi_uncert\"]}\n",
    "\n",
    "for i in range(num_steps):\n",
    "    print(f\"step {i}: loss={objective}\")\n",
    "    data = next(batch_iterator)\n",
    "    start = perf_counter()\n",
    "    params, state = solver.update(\n",
    "        params, state, bins=bins, data=data, loss=objective, bandwidth=9e-2\n",
    "    )\n",
    "    end = perf_counter()\n",
    "    print(f\"update took {end-start:.4g}s\")\n",
    "    if \"bins\" in params:\n",
    "        print(\"bin edges: [0 \", *[f\"{f:.3g}\" for f in params[\"bins\"]], \" 1]\")\n",
    "    for metric in metrics:\n",
    "        test_metric = loss(params, bins=bins, data=test, loss=metric, bandwidth=1e-8)\n",
    "        print(f\"{metric}={test_metric:.4g}\")\n",
    "        metrics[metric].append(test_metric)\n",
    "    if metrics[\"discovery\"][-1] < best_sig:\n",
    "        best_params = params\n",
    "        best_sig = metrics[\"discovery\"][-1]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.00066241, dtype=float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "22d6333b89854cd01c2018f3ca2f5a59a2cde2765fbca789ff36cfad48ca629b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
